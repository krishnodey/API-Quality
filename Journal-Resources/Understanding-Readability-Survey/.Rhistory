ratings_long <- ratings_long %>%
arrange(factor(rule, levels = rev(p))) %>%
mutate(index = row_number())
#print(ratings_long)
# create break values to avoid negative numbers
break_values <- append(pretty(ratings_long$value), c(30, 40, 50))
# create the plot
ggplot(ratings_long, aes(x = reorder(rule, index), y = value, fill = variable)) +
geom_bar(stat = "identity") +
theme_classic() +
# hide y axis
theme(
axis.line.y = element_blank(),
axis.ticks.y = element_blank()
) +
labs(x = "", y = "# of readability ratings per difficulty level", fill = "") +
# set absolute numbers to avoid negatives and stretch axis the same in both directions
scale_y_continuous(
limits = c(-51, 51),
breaks = break_values,
labels = abs(break_values)
) +
# colors for the 4 counts
scale_fill_manual(values = c("#0077dd", "lightblue", "#dd1100", "#ff7e73")) +
# bar label text and positioning
geom_text(
aes(label = abs(value)),
position = position_stack(vjust = 0.5),
fontface = "bold",
size = 5.5,
color = "white"
) +
# add vertical line in the middle
geom_hline(yintercept = 0, size = 0.75) +
theme(
text = element_text(size = 16, face = "bold", family = "sans"),
axis.title = element_text(size = 18),
axis.text.x = element_text(size = 16),
axis.text.y = element_text(size = 16),
legend.text = element_text(size = 16),
legend.position = "top"
) +
# flip the chart horizontally
coord_flip()
# start hypothesis testing
# due to multiple hypotheses, we will use the Holm-Bonferroni correction to adjust the computed p-values; the confidence level is therefore set to 0.05 here
confLevel <- 0.05
# calculate Wilcoxon–Mann–Whitney test
# standard implementation from the `stats` package (asymptotic approximation with ties)
testResults <- data.frame()
# for individual rules
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varRatingP <- paste(p, "RRating", sep = "")
varRatingAP <- paste(ap, "RRating", sep = "")
w <- wilcox.test(
x = data[[varRatingAP]],
y = data[[varRatingP]],
alternative = "greater",
conf.level = confLevel
)
# calculate the effect size with Cohens d
d <- cohens_d(data[[varRatingAP]], data[[varRatingP]])
#print(d)
# create results data frame
rule <- c(p)
U.value <- c(w$statistic)
p.value <- c(w$p.value)
cohens.d <- c(d$Cohens_d)
testResults <- rbind(testResults,
data.frame(rule, U.value, p.value, cohens.d),
make.row.names = FALSE
)
}
print(testResults)
# adjust p-values with Holm-Bonferroni and format them
testResults <- testResults %>%
mutate(p.value = format.pval(
p.adjust(p.value, method = "holm"),
digits = 4, eps = 0.001
)
)
print(testResults)
# for all rules combined
w <- wilcox.test(
x = combinedReadDf$Pattern_rating,
y = combinedReadDf$Antipattern_rating,
alternative = "greater",
conf.level = confLevel
)
d <- cohens_d(combinedReadDf$Pattern_rating, combinedReadDf$Antipattern_rating)
rule <- c("All rules combined")
U.value <- c(w$statistic)
p.value <- c(format.pval(w$p.value, digits = 4, eps = 0.001))
cohens.d <- c(d$Cohens_d)
testResults <- rbind(testResults,
data.frame(rule, U.value, p.value, cohens.d),
make.row.names = FALSE
)
print(testResults)
# visualize d values with bar plot
barplotData <- testResults %>%
filter(rule != "All rules combined")
# use ggplot to print the bar chart
ggplot(barplotData, aes(x = reorder(rule, cohens.d), y = cohens.d)) +
geom_bar(
stat = "identity",
width = 0.8,
fill = "#0077dd"
) +
# control space for the labels
scale_y_continuous(expand = expansion(mult = c(0, .1))) +
# bar label text and positioning
geom_text(
aes(label = sprintf("%.2f", round(cohens.d, 2))),
size = 5.1,
fontface = "bold",
hjust = -0.05
) +
theme_classic() +
labs(x = "", y = "Cohen's d") +
theme(
text = element_text(size = 16, face = "bold", family = "sans"),
axis.title = element_text(size = 16),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 14),
legend.title = element_text(size = 16),
legend.text = element_text(size = 16)
) +
# flip the chart horizontally
coord_flip()
# calculate correlation between perceived readability and actual understandability (TAU)
correlationReadResults <- data.frame()
corrMthd <- "kendall"
# for each individual rule
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varRatingP <- paste(p, "RRating", sep = "")
varRatingAP <- paste(ap, "RRating", sep = "")
varTAUP <- paste("TAU", p, sep = "_")
varTAUAP <- paste("TAU", ap, sep = "_")
corP <- cor.test(
data[[varRatingP]],
data[[varTAUP]],
method = corrMthd,
alternative = "less",
conf.level = confLevel
)
corAP <- cor.test(
data[[varRatingAP]],
data[[varTAUAP]],
method = corrMthd,
alternative = "less",
conf.level = confLevel
)
correlationReadResults <- rbind(correlationReadResults,
data.frame(
rule = p,
P_correlation = corP$estimate,
P_p.value = corP$p.value,
AP_correlation = corAP$estimate,
AP_p.value = corAP$p.value
),
make.row.names = FALSE
)
}
print(correlationReadResults)
# adjust p-values with Holm-Bonferroni and format them
correlationReadResults <- correlationReadResults %>%
mutate(P_p.value = format.pval(
p.adjust(P_p.value, method = "holm"),
digits = 4, eps = 0.001
)
) %>%
mutate(AP_p.value = format.pval(
p.adjust(AP_p.value, method = "holm"),
digits = 4, eps = 0.001
)
)
print(correlationReadResults)
# for all rules combined
corFR <- cor.test(
combinedReadDf$Pattern_rating,
combinedReadDf$Pattern_TAU,
method = corrMthd,
alternative = "less",
conf.level = confLevel
)
corIR <- cor.test(
combinedReadDf$Antipattern_rating,
combinedReadDf$Antipattern_TAU,
method = corrMthd,
alternative = "less",
conf.level = confLevel
)
correlationReadResults <- rbind(correlationReadResults,
data.frame(
rule = "All rules combined",
P_correlation = corP$estimate,
P_p.value = format.pval(corP$p.value, digits = 4, eps = 0.001),
AP_correlation = corAP$estimate,
AP_p.value = format.pval(corAP$p.value, digits = 4, eps = 0.001)
),
make.row.names = FALSE
)
print(correlationReadResults)
# regression results for each individual rule
df <- data.frame()
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varRatingP <- paste(p, "RRating", sep = "")
varRatingAP <- paste(ap, "RRating", sep = "")
varTAUP <- paste("TAU", p, sep = "_")
varTAUAP <- paste("TAU", ap, sep = "_")
resP <- lm(
as.formula(paste(varTAUP, "~", varRatingP)),
data = data
)
resAP <- lm(
as.formula(paste(varTAUAP, "~", varRatingAP)),
data = data
)
df <- rbind(df,
data.frame(
rule = ap,
P_adjustedR2 = summary(resP)$adj.r.squared,
P_p.value = format.pval(summary(resP)$coefficients[8], digits = 4, eps = 0.001),
AP_adjustedR2 = summary(resAP)$adj.r.squared,
AP_p.value = format.pval(summary(resAP)$coefficients[8], digits = 4, eps = 0.001)
),
make.row.names = FALSE
)
}
print(df)
df %>% arrange(AP_adjustedR2) %>%
print
# visualize adjusted R-squared values for violation
barplotData <- df %>%
select(rule, AP_adjustedR2)
# use ggplot to print the bar chart
ggplot(barplotData, aes(x = reorder(rule, AP_adjustedR2), y = AP_adjustedR2)) +
geom_bar(
stat = "identity",
width = 0.8,
fill = "#dd1100"
) +
# control space for the labels
scale_y_continuous(expand = expansion(mult = c(0.01, 0.12))) +
# bar label text and positioning
geom_text(
aes(
label = sprintf("%.2f", round(AP_adjustedR2, 2)),
# fix label placement for negative numbers
y = ifelse(AP_adjustedR2 > 0, AP_adjustedR2 + 0.001, AP_adjustedR2 - 0.025)
),
size = 5,
fontface = "bold",
hjust = -0.05
) +
theme_classic() +
labs(x = "", y = "Adjusted R-squared") +
theme(
text = element_text(size = 16, face = "bold", family = "sans"),
axis.title = element_text(size = 16),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 14),
legend.title = element_text(size = 16),
legend.text = element_text(size = 16)
) +
# flip the chart horizontally
coord_flip()
#########################################################################################################
# Calculations for RQ4 (How do participant demographics influence the understandability and readability of Web APIs?)
#########################################################################################################
# aggregate mean values for TAU, perceived understandability, correctness, and time (needed for RQ4)
correlationUndData <- data %>%
rowwise() %>%
mutate(
mean_TAU_P =
mean(c(TAU_TidyEndpoint, TAU_ContextualResource, TAU_VerblessEndpoint, TAU_ConsistentDoc, TAU_DescriptiveEndpoint, TAU_HierarchicalNodes, TAU_PertinentDoc), na.rm = TRUE),
mean(c(TAU_StandardEndpoint, TAU_SingularizedNodes, TAU_VersionedEndpoint, TAU_ParameterAdherence, TAU_ConsistentArchetype, TAU_IdentifierAnnotation, TAU_StructuredEndpoint), na.rm = TRUE)
) %>%
mutate(
mean_TAU_AP =
mean(c(TAU_AmorphousEndpoint, TAU_ContextlessResource, TAU_CRUDyEndpoint, TAU_InconsistentDoc, TAU_NonDescriptiveEndpoint, TAU_NonHierarchicalNodes, TAU_NonPertinentDoc), na.rm = TRUE),
mean(c(TAU_NonStandardEndpoint, TAU_PluralizedNodes, TAU_UnversionedEndpoint, TAU_ParameterTunneling, TAU_InconsistentArchetype, TAU_IdentifierAmbiguity, TAU_FlatEndpoint), na.rm = TRUE)
) %>%
mutate(
mean_perceivedUnderstandabilityDifficulty_P =
mean(c(TidyEndpointURating, ContextualResourceURating, VerblessEndpointURating, ConsistentDocURating, DescriptiveEndpointURating, HierarchicalNodesURating, PertinentDocURating), na.rm = TRUE),
mean(c(StandardEndpointURating, SingularizedNodesURating, VersionedEndpointURating, ParameterAdherenceURating, ConsistentArchetypeURating, IdentifierAnnotationURating, StructuredEndpointURating), na.rm = TRUE)
) %>%
mutate(
mean_perceivedUnderstandabilityDifficulty_AP =
mean(c(AmorphousEndpointURating, ContextlessResourceURating, CRUDyEndpointURating, InconsistentDocURating, NonDescriptiveEndpointURating, NonHierarchicalNodesURating, NonPertinentDocURating), na.rm = TRUE),
mean(c(NonStandardEndpointURating, PluralizedNodesURating, UnversionedEndpointURating, ParameterTunnelingURating, InconsistentArchetypeURating, IdentifierAmbiguityURating, FlatEndpointURating), na.rm = TRUE)
) %>%
mutate(
mean_Time_P =
mean(c(TidyEndpointTime, ContextualResourceURating, VerblessEndpointTime, ConsistentDocTime, DescriptiveEndpointTime, HierarchicalNodesTime, PertinentDocTime), na.rm = TRUE),
mean(c(StandardEndpointTime, SingularizedNodesTime, VersionedEndpointTime, ParameterAdherenceTime, ConsistentArchetypeTime, IdentifierAnnotationTime, StructuredEndpointTime), na.rm = TRUE)
) %>%
mutate(
mean_Time_AP =
mean(c(AmorphousEndpointTime, ContextlessResourceTime, CRUDyEndpointTime, InconsistentDocTime, NonDescriptiveEndpointTime, NonHierarchicalNodesTime, NonPertinentDocTime), na.rm = TRUE),
mean(c(NonStandardEndpointTime, PluralizedNodesTime, VersionedEndpointTime, ParameterTunnelingTime, InconsistentArchetypeTime, IdentifierAmbiguityTime, FlatEndpointTime), na.rm = TRUE)
) %>%
mutate(
mean_Corr_P =
mean(c(TidyEndpoint, ContextualResource, VerblessEndpoint, ConsistentDoc, DescriptiveEndpoint, HierarchicalNodes, PertinentDoc), na.rm = TRUE),
mean(c(StandardEndpoint, SingularizedNodes, VersionedEndpoint, ParameterAdherence, ConsistentArchetype, IdentifierAnnotation, StructuredEndpoint), na.rm = TRUE)
) %>%
mutate(
mean_Corr_AP =
mean(c(AmorphousEndpoint, ContextlessResource, CRUDyEndpoint, InconsistentDoc, NonDescriptiveEndpoint, NonHierarchicalNodes, NonPertinentDoc), na.rm = TRUE),
mean(c(NonStandardEndpoint, PluralizedNodes, UnversionedEndpoint, ParameterTunneling, InconsistentArchetype, IdentifierAmbiguity, FlatEndpoint), na.rm = TRUE)
)
print(correlationUndData)
# Correlation matrix for exploration
# rcorr does not support kendalls TAU
# "pearson" for linear correlation of continuous variables
# "spearman" for rank-based monotonic correlation (with ordinal variables)
corrMthd <- "spearman"
corrMatrix <-
correlationUndData %>%
select(
starts_with("mean_"),
starts_with("is_"),
YearsOfExperience,
RichardsonMaturity,
MaturityLevel
) %>%
as.matrix() %>%
rcorr(type = corrMthd)
print(corrMatrix)
corrplot(corrMatrix$r, p.mat = corrMatrix$P, method = "circle", type = "lower")
# caculate separate correlations
corrMthd <- "kendall"
df <- data.frame()
# change to "mean_TAU_P", "mean_TAU_AP", "mean_perceivedUnderstandabilityDifficulty_P", or "mean_perceivedUnderstandabilityDifficulty_AP"
dependent <- "mean_TAU_AP"
demographics <- c("is_Student", "is_Academia", "is_Canada", "YearsOfExperience", "RichardsonMaturity", "MaturityLevel")
for (d in demographics) {
res <- cor.test(
correlationUndData[[dependent]],
correlationUndData[[d]],
method = corrMthd,
exact = FALSE
)
df <- rbind(df, data.frame(
dependent = dependent,
demographic = d,
estimate = res$estimate,
p.value = res$p.value
),
make.row.names = FALSE
)
}
print(df)
# adjust p-values with Holm-Bonferroni and format them
df %>%
mutate(p.value = format.pval(
p.adjust(p.value, method = "holm"),
digits = 4, eps = 0.001
)
) %>%
arrange(p.value) %>%
print
# create 2 linear regression models for predicting TAU (one for following patterns and one for followng antipatterns)
# LM for rule:
lrP <- lm(
mean_TAU_P ~
is_Student
+ is_Academia
+ is_Canada
+ YearsOfExperience
+ MaturityLevel,
data = correlationUndData
)
summary(lrP)
# LM for violation:
lrAP <- lm(
mean_TAU_AP ~
is_Student
+ is_Academia
+ is_Canada
+ YearsOfExperience
+ MaturityLevel,
data = correlationUndData
)
summary(lrAP)
# aggregate mean values for TAU, perceived readability, correctness, and time (needed for RQ3)
correlationReadData <- data %>%
rowwise() %>%
mutate(
mean_TAU_P =
mean(c(TAU_TidyEndpoint, TAU_ContextualResource, TAU_VerblessEndpoint, TAU_ConsistentDoc, TAU_DescriptiveEndpoint, TAU_HierarchicalNodes, TAU_PertinentDoc), na.rm = TRUE),
mean(c(TAU_StandardEndpoint, TAU_SingularizedNodes, TAU_VersionedEndpoint, TAU_ParameterAdherence, TAU_ConsistentArchetype, TAU_IdentifierAnnotation, TAU_StructuredEndpoint), na.rm = TRUE)
) %>%
mutate(
mean_TAU_AP =
mean(c(TAU_AmorphousEndpoint, TAU_ContextlessResource, TAU_CRUDyEndpoint, TAU_InconsistentDoc, TAU_NonDescriptiveEndpoint, TAU_NonHierarchicalNodes, TAU_NonPertinentDoc), na.rm = TRUE),
mean(c(TAU_NonStandardEndpoint, TAU_PluralizedNodes, TAU_UnversionedEndpoint, TAU_ParameterTunneling, TAU_InconsistentArchetype, TAU_IdentifierAmbiguity, TAU_FlatEndpoint), na.rm = TRUE)
) %>%
mutate(
mean_perceivedReadabilityDifficulty_P =
mean(c(TidyEndpointRRating, ContextualResourceRRating, VerblessEndpointRRating, ConsistentDocRRating, DescriptiveEndpointRRating, HierarchicalNodesRRating, PertinentDocRRating), na.rm = TRUE),
mean(c(StandardEndpointRRating, SingularizedNodesRRating, VersionedEndpointRRating, ParameterAdherenceRRating, ConsistentArchetypeRRating, IdentifierAnnotationRRating, StructuredEndpointRRating), na.rm = TRUE)
) %>%
mutate(
mean_perceivedReadabilityDifficulty_AP =
mean(c(AmorphousEndpointRRating, ContextlessResourceRRating, CRUDyEndpointRRating, InconsistentDocRRating, NonDescriptiveEndpointRRating, NonHierarchicalNodesRRating, NonPertinentDocRRating), na.rm = TRUE),
mean(c(NonStandardEndpointRRating, PluralizedNodesRRating, UnversionedEndpointRRating, ParameterTunnelingRRating, InconsistentArchetypeRRating, IdentifierAmbiguityRRating, FlatEndpointRRating), na.rm = TRUE)
) %>%
mutate(
mean_Time_P =
mean(c(TidyEndpointTime, ContextualResourceURating, VerblessEndpointTime, ConsistentDocTime, DescriptiveEndpointTime, HierarchicalNodesTime, PertinentDocTime), na.rm = TRUE),
mean(c(StandardEndpointTime, SingularizedNodesTime, VersionedEndpointTime, ParameterAdherenceTime, ConsistentArchetypeTime, IdentifierAnnotationTime, StructuredEndpointTime), na.rm = TRUE)
) %>%
mutate(
mean_Time_AP =
mean(c(AmorphousEndpointTime, ContextlessResourceTime, CRUDyEndpointTime, InconsistentDocTime, NonDescriptiveEndpointTime, NonHierarchicalNodesTime, NonPertinentDocTime), na.rm = TRUE),
mean(c(NonStandardEndpointTime, PluralizedNodesTime, VersionedEndpointTime, ParameterTunnelingTime, InconsistentArchetypeTime, IdentifierAmbiguityTime, FlatEndpointTime), na.rm = TRUE)
) %>%
mutate(
mean_Corr_P =
mean(c(TidyEndpoint, ContextualResource, VerblessEndpoint, ConsistentDoc, DescriptiveEndpoint, HierarchicalNodes, PertinentDoc), na.rm = TRUE),
mean(c(StandardEndpoint, SingularizedNodes, VersionedEndpoint, ParameterAdherence, ConsistentArchetype, IdentifierAnnotation, StructuredEndpoint), na.rm = TRUE)
) %>%
mutate(
mean_Corr_AP =
mean(c(AmorphousEndpoint, ContextlessResource, CRUDyEndpoint, InconsistentDoc, NonDescriptiveEndpoint, NonHierarchicalNodes, NonPertinentDoc), na.rm = TRUE),
mean(c(NonStandardEndpoint, PluralizedNodes, UnversionedEndpoint, ParameterTunneling, InconsistentArchetype, IdentifierAmbiguity, FlatEndpoint), na.rm = TRUE)
)
print(correlationReadData)
# Correlation matrix for exploration
# rcorr does not support kendalls TAU
# "pearson" for linear correlation of continuous variables
# "spearman" for rank-based monotonic correlation (with ordinal variables)
corrMthd <- "spearman"
corrMatrix <-
correlationReadData %>%
select(
starts_with("mean_"),
starts_with("is_"),
YearsOfExperience,
RichardsonMaturity,
MaturityLevel
) %>%
as.matrix() %>%
rcorr(type = corrMthd)
#print(corrMatrix)
# plot a correlation matrix
corrplot(corrMatrix$r, p.mat = corrMatrix$P, method = "circle", type = "lower")
# caculate separate correlations
corrMthd <- "kendall"
df <- data.frame()
# change to "mean_TAU_P", "mean_TAU_AP", "mean_perceivedReadabilityDifficulty_P", or "mean_perceivedReadabilityDifficulty_AP"
dependent <- "mean_TAU_P"
demographics <- c("is_Student", "is_Academia", "is_Canada", "YearsOfExperience", "RichardsonMaturity", "MaturityLevel")
for (d in demographics) {
res <- cor.test(
correlationReadData[[dependent]],
correlationReadData[[d]],
method = corrMthd,
exact = FALSE
)
df <- rbind(df, data.frame(
dependent = dependent,
demographic = d,
estimate = res$estimate,
p.value = res$p.value
),
make.row.names = FALSE
)
}
print(df)
# adjust p-values with Holm-Bonferroni and format them
df %>%
mutate(p.value = format.pval(
p.adjust(p.value, method = "holm"),
digits = 4, eps = 0.001
)
) %>%
arrange(p.value) %>%
print
# create 2 linear regression models for predicting TAU (one for following patterns and one for followng antipatterns)
# LM for rule:
lrP <- lm(
mean_TAU_P ~
is_Student
+ is_Academia
+ is_Canada
+ YearsOfExperience
+ MaturityLevel,
data = correlationReadData
)
summary(lrP)
# LM for violation:
lrAP <- lm(
mean_TAU_AP ~
is_Student
+ is_Academia
+ is_Canada
+ YearsOfExperience
+ MaturityLevel,
data = correlationReadData
)
summary(lrAP)
