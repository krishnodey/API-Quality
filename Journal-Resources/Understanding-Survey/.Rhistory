setwd("~/API-Quality/Journal-Resources/Understanding-Survey")
########################################################################
# Experiment Results
########################################################################
# delete current environment variables --------
rm(list = ls(all.names = TRUE))
# install and load required packages -------
# (taken from https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)
packageList <- c(
"Hmisc", "dplyr", "ggplot2", "tidyr", "corrplot", "likert", "effectsize", "scales", "lmtest"
)
newPackages <- packageList[!(packageList %in% installed.packages()[, "Package"])]
if (length(newPackages)) install.packages(newPackages)
for (p in packageList) {
library(p, character.only = TRUE)
}
# info: the abbreviation `FR` stands for "Following Rule", so everything annotated with FR addresses the version that
# adheres to the rules. `IR` stands for "Ignoring Rule", i.e., this addresses the version that violates the rules.
# helper function to visually and statistically check the distribution of a data set
checkDataDistribution <- function(data, xaxis.title) {
hist(data, xlab = xaxis.title)
# Shapiro-Wilk test for non-normal distribution
# Null hypothesis with Shapiro-Wilk test is that "data" came from a normally distributed population,
# i.e. p-value <= 0.05 --> "data" is not normally distributed
shapiro.test(data)
}
# helper function to calculate the Timed Actual Understandability (TAU)
calculateTAU <- function(effectiveness, efficiency, maxEfficiency) {
effectiveness * (1 - (efficiency / maxEfficiency))
}
# read data ------------
data <- read.csv("results.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# distribution of groups
data %>%
select(group_id) %>%
table()
# median participant stats by task order group
data %>%
group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numGermany = sum(is_Germany)
)
# variable names for the rules
ruleNames <- c(
"PluralNoun", "VerbController", "CRUDNames",
"PathHierarchy1", "PathHierarchy2", "PathHierarchy3",
"NoTunnel", "GETRetrieve", "POSTCreate",
"NoRC200Error", "RC401", "RC415"
)
# calculate TAU for every question and both groups
for (var in ruleNames) {
var <- as.name(var)
varFR <- as.name(paste(var, "FR", sep = ""))
varFRTime <- as.name(paste(var, "FRTime", sep = ""))
varIR <- as.name(paste(var, "IR", sep = ""))
varIRTime <- as.name(paste(var, "IRTime", sep = ""))
data <- data %>%
mutate("TAUFR_{{var}}" := calculateTAU({{ varFR }}, {{ varFRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE))) %>%
mutate("TAUIR_{{var}}" := calculateTAU({{ varIR }}, {{ varIRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)))
}
print(data)
# --> data now has attributes TAUFR_<RuleIdentifier> and TAUIR_<RuleIdentifier>
# create custom data frame with ratings and TAU for all rules combined
combinedDf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(combinedDf) <- c("FR_rating", "FR_TAU", "IR_rating", "IR_TAU")
for (var in ruleNames) {
varRatingFR <- paste(var, "FR_rating", sep = "")
varRatingIR <- paste(var, "IR_rating", sep = "")
varTAUFR <- paste("TAUFR", var, sep = "_")
varTAUIR <- paste("TAUIR", var, sep = "_")
# merge data frames
combinedDf <- rbind(combinedDf, data.frame(
FR_rating = data[[varRatingFR]],
FR_TAU = data[[varTAUFR]],
IR_rating = data[[varRatingIR]],
IR_TAU = data[[varTAUIR]]
))
}
print(combinedDf)
# Shapiro-Wilk test for non-normal distribution (replace value with the different rule identifiers, i.e., 1 to 12)
var <- ruleNames[1]
varFR <- as.name(paste("TAUFR", var, sep = "_"))
varIR <- as.name(paste("TAUIR", var, sep = "_"))
checkDataDistribution(data[[varFR]], varFR)
checkDataDistribution(data[[varIR]], varIR)
# calculate descriptive statistics for perceived understandability ratings
descriptiveStats <- data.frame()
for (var in ruleNames) {
varTAUFR <- as.name(paste("TAUFR", var, sep = "_"))
varTAUIR <- as.name(paste("TAUIR", var, sep = "_"))
varFR <- as.name(paste(var, "FR", sep = ""))
varIR <- as.name(paste(var, "IR", sep = ""))
varFR_rating <- as.name(paste(var, "FR_rating", sep = ""))
varIR_rating <- as.name(paste(var, "IR_rating", sep = ""))
descriptiveStats <- rbind(
descriptiveStats,
data %>% summarise(
rule = var,
median_rating_FR := median({{ varFR_rating }}, na.rm = TRUE),
median_rating_IR := median({{ varIR_rating }}, na.rm = TRUE),
mean_rating_FR := round(mean({{ varFR_rating }}, na.rm = TRUE), 2),
mean_rating_IR := round(mean({{ varIR_rating }}, na.rm = TRUE), 2),
mean_TAU_FR := mean({{ varTAUFR }}, na.rm = TRUE),
mean_TAU_IR := mean({{ varTAUIR }}, na.rm = TRUE),
correctPercent_FR := scales::percent(sum({{ varFR }}, na.rm = TRUE) / (data %>% filter(!is.na({{ varFR }})) %>% nrow())),
correctPercent_IR := scales::percent(sum({{ varIR }}, na.rm = TRUE) / (data %>% filter(!is.na({{ varIR }})) %>% nrow()))
)
)
}
print(descriptiveStats)
# create custom likert plots for perceived understandability ratings
ratings <- data.frame()
for (var in ruleNames) {
var <- as.name(var)
varRatingFR <- as.name(paste(var, "FR_rating", sep = ""))
varRatingIR <- as.name(paste(var, "IR_rating", sep = ""))
FR1 <- data %>% filter({{ varRatingFR }} == 1) %>% nrow() * -1
FR2 <- data %>% filter({{ varRatingFR }} == 2) %>% nrow() * -1
IR1 <- data %>% filter({{ varRatingIR }} == 1) %>% nrow()
IR2 <- data %>% filter({{ varRatingIR }} == 2) %>% nrow()
ratings <- rbind(ratings,
data.frame(
rule = as.character(var),
countFR1 = FR1,
countFR2 = FR2,
countIR2 = IR2,
countIR1 = IR1
)
)
}
print(ratings)
ratings_long <- reshape2::melt(ratings, id.vars = "rule")
ratings_long$variable <- factor(
ratings_long$variable,
levels = c("countFR1", "countFR2", "countIR1", "countIR2"),
labels = c("Very easy (rule)", "Easy (rule)", "Very easy (violation)", "Easy (violation)")
)
print(ratings_long)
print(ruleNames)
# order according to categories
ratings_long <- ratings_long %>%
arrange(factor(rule, levels = rev(ruleNames))) %>%
mutate(index = row_number())
print(ratings_long)
# create break values to avoid negative numbers
break_values <- append(pretty(ratings_long$value), c(30, 40, 50))
print(break_values)
# create the plot
ggplot(ratings_long, aes(x = reorder(rule, index), y = value, fill = variable)) +
geom_bar(stat = "identity") +
theme_classic() +
# hide y axis
theme(
axis.line.y = element_blank(),
axis.ticks.y = element_blank()
) +
labs(x = "Rule", y = "# of ratings per difficulty level", fill = "") +
# set absolute numbers to avoid negatives and stretch axis the same in both directions
scale_y_continuous(
limits = c(-51, 51),
breaks = break_values,
labels = abs(break_values)
) +
# colors for the 4 counts
scale_fill_manual(values = c("#0077dd", "lightblue", "#dd1100", "#ff7e73")) +
# bar label text and positioning
geom_text(
aes(label = abs(value)),
position = position_stack(vjust = 0.5),
fontface = "bold",
size = 5.5,
color = "white"
) +
# add vertical line in the middle
geom_hline(yintercept = 0, size = 0.75) +
theme(
text = element_text(size = 16, face = "bold", family = "sans"),
axis.title = element_text(size = 18),
axis.text.x = element_text(size = 16),
axis.text.y = element_text(size = 16),
legend.text = element_text(size = 16),
legend.position = "top"
) +
# flip the chart horizontally
coord_flip()
# start hypothesis testing
# due to multiple hypotheses, we will use the Holm-Bonferroni correction to adjust the computed p-values; the confidence level is therefore set to 0.05 here
confLevel <- 0.05
# calculate Wilcoxon–Mann–Whitney test
# standard implementation from the `stats` package (asymptotic approximation with ties)
testResults <- data.frame()
# for individual rules
for (var in ruleNames) {
varRatingFR <- paste(var, "FR_rating", sep = "")
varRatingIR <- paste(var, "IR_rating", sep = "")
w <- wilcox.test(
x = data[[varRatingIR]],
y = data[[varRatingFR]],
alternative = "greater",
conf.level = confLevel
)
# calculate the effect size with Cohens d
d <- cohens_d(data[[varRatingIR]], data[[varRatingFR]])
# create results data frame
rule <- c(var)
U.value <- c(w$statistic)
p.value <- c(w$p.value)
cohens.d <- c(d$Cohens_d)
testResults <- rbind(testResults,
data.frame(rule, U.value, p.value, cohens.d),
make.row.names = FALSE
)
}
print(testResults)
# adjust p-values with Holm-Bonferroni and format them
testResults <- testResults %>%
mutate(p.value = format.pval(
p.adjust(p.value, method = "holm"),
digits = 4, eps = 0.001
)
)
print(combinedDf)
# for all rules combined
w <- wilcox.test(
x = combinedDf$IR_rating,
y = combinedDf$FR_rating,
alternative = "greater",
conf.level = confLevel
)
d <- cohens_d(combinedDf$IR_rating, combinedDf$FR_rating)
rule <- c("All rules combined")
U.value <- c(w$statistic)
p.value <- c(format.pval(w$p.value, digits = 4, eps = 0.001))
cohens.d <- c(d$Cohens_d)
testResults <- rbind(testResults,
data.frame(rule, U.value, p.value, cohens.d),
make.row.names = FALSE
)
print(testResults)
# visualize d values with bar plot
barplotData <- testResults %>%
filter(rule != "All rules combined" & rule != "PathHierarchy3" &
rule != "VerbController" & rule != "RC415")
# use ggplot to print the bar chart
ggplot(barplotData, aes(x = reorder(rule, cohens.d), y = cohens.d)) +
geom_bar(
stat = "identity",
width = 0.8,
fill = "#0077dd"
) +
# control space for the labels
scale_y_continuous(expand = expansion(mult = c(0, .1))) +
# bar label text and positioning
geom_text(
aes(label = sprintf("%.2f", round(cohens.d, 2))),
size = 5.1,
fontface = "bold",
hjust = -0.05
) +
theme_classic() +
labs(x = "Rule", y = "Cohen's d") +
theme(
text = element_text(size = 16, face = "bold", family = "sans"),
axis.title = element_text(size = 16),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 14),
legend.title = element_text(size = 16),
legend.text = element_text(size = 16)
) +
# flip the chart horizontally
coord_flip()
# calculate correlation between perceived understandability and actual understandability (TAU)
correlationResults <- data.frame()
corrMthd <- "kendall"
# for each individual rule
for (var in ruleNames) {
varRatingFR <- paste(var, "FR_rating", sep = "")
varRatingIR <- paste(var, "IR_rating", sep = "")
varTAUFR <- paste("TAUFR", var, sep = "_")
varTAUIR <- paste("TAUIR", var, sep = "_")
corFR <- cor.test(
data[[varRatingFR]],
data[[varTAUFR]],
method = corrMthd,
alternative = "less",
conf.level = confLevel
)
corIR <- cor.test(
data[[varRatingIR]],
data[[varTAUIR]],
method = corrMthd,
alternative = "less",
conf.level = confLevel
)
correlationResults <- rbind(correlationResults,
data.frame(
rule = var,
FR_correlation = corFR$estimate,
FR_p.value = corFR$p.value,
IR_correlation = corIR$estimate,
IR_p.value = corIR$p.value
),
make.row.names = FALSE
)
}
print(correlationResults)
# adjust p-values with Holm-Bonferroni and format them
correlationResults <- correlationResults %>%
mutate(FR_p.value = format.pval(
p.adjust(FR_p.value, method = "holm"),
digits = 4, eps = 0.001
)
) %>%
mutate(IR_p.value = format.pval(
p.adjust(IR_p.value, method = "holm"),
digits = 4, eps = 0.001
)
)
print(correlationResults)
