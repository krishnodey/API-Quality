setwd("~/API-Quality/Journal-Resources/Understanding-Survey")
########################################################################
# Experiment Results
########################################################################
# delete current environment variables --------
rm(list = ls(all.names = TRUE))
# install and load required packages -------
# (taken from https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)
packageList <- c(
"Hmisc", "dplyr", "ggplot2", "tidyr", "corrplot", "likert", "effectsize", "scales", "lmtest"
)
newPackages <- packageList[!(packageList %in% installed.packages()[, "Package"])]
if (length(newPackages)) install.packages(newPackages)
for (p in packageList) {
library(p, character.only = TRUE)
}
# helper function to visually and statistically check the distribution of a data set
checkDataDistribution <- function(data, xaxis.title) {
hist(data, xlab = xaxis.title)
# Shapiro-Wilk test for non-normal distribution
# Null hypothesis with Shapiro-Wilk test is that "data" came from a normally distributed population,
# i.e. p-value <= 0.05 --> "data" is not normally distributed
shapiro.test(data)
}
# helper function to calculate the Timed Actual Understandability (TAU)
calculateTAU <- function(effectiveness, efficiency, maxEfficiency) {
effectiveness * (1 - (efficiency / maxEfficiency))
}
# read data ------------
data <- read.csv("results-survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
print(data)
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# distribution of groups
#data %>%
#  select(group_id) %>%
#  table()
# median participant stats by task order group
data %>%
#group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numDeveloper = sum(is_Developer),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numCanada = sum(is_Canada)
)
# variable names for the antipatterns
antipatterns <- c(
"AmorphousEndpoint", "ContextlessResource", "CRUDyEndpoint",
"InconsistentEndpoint", "NonDescriptiveEndpoint", "NonHierarchicalEndpoint",
"NonPertinentEndpoint", "NonStandardEndpoint", "PluralizedEndpoint",
"UnversionedEndpoint", "ParameterTunneling", "InconsistentArchetype", "IdentifierAmbiguity", "FlatEndpoint"
)
# variable names for the patterns
patterns <- c(
"TidyEndpoint", "ContextualResource", "VerblessEndpoint",
"ConsistentEndpoint", "DescriptiveEndpoint", "HierarchicalEndpoint",
"PertinentEndpoint", "StandardEndpoint", "SingularizedEndpoint",
"VersionedEndpoint", "ParameterAdherence", "ConsistentArchetype", "IdentifierAnnotation", "StructuredEndpoint"
)
# calculate TAU for every questions
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varPattern <- as.name(p)
varPatternTime <- as.name(paste(p, "Time", sep = ""))
varAntipattern <- as.name(ap)
varAntipatternTime <- as.name(paste(ap, "Time", sep = ""))
data <- data %>%
mutate("TAU_{{varPattern}}" := calculateTAU({{ varPattern }}, {{ varPatternTime }}, max({{ varPatternTime }}, {{ varAntipatternTime }}, na.rm = TRUE))) %>%
mutate("TAU_{{varAntipattern}}" := calculateTAU({{ varAntipattern }}, {{ varAntipatternTime }}, max({{ varPatternTime }}, {{ varAntipatternTime }}, na.rm = TRUE)))
}
print(data)
# --> data now has attributes TAU_<Pattern> and TAU_<Antipattern>
# create custom data frame with understandability ratings and TAU for all rules combined
combinedUndDf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(combinedUndDf) <- c("Pattern_rating_Und", "Pattern_TAU", "Antipattern_rating_Und", "Antipattern_TAU")
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varRatingPattern <- paste(p, "URating", sep = "")
varRatingAntipattern <- paste(ap, "URating", sep = "")
varTAUPattern <- paste("TAU", p, sep = "_")
varTAUAntipattern <- paste("TAU", ap, sep = "_")
#print(varRatingAntipattern)
#print(varTAUPattern)
# merge data frames
combinedUndDf <- rbind(combinedUndDf, data.frame(
Pattern_rating = data[[varRatingPattern]],
Pattern_TAU = data[[varTAUPattern]],
Antipattern_rating = data[[varRatingAntipattern]],
Antipattern_TAU = data[[varTAUAntipattern]]
))
}
print(combinedUndDf)
# Shapiro-Wilk test for non-normal distribution of understandability (replace value with the different rule identifiers, i.e., 1 to 12)
p <- patterns[1]
ap <- antipatterns[1]
varP <- as.name(paste("TAU", p, sep = "_"))
print(varP)
varAP <- as.name(paste("TAU", ap, sep = "_"))
print(varAP)
checkDataDistribution(data[[varP]], varP)
checkDataDistribution(data[[varAP]], varAP)
# create custom data frame with readability ratings and TAU for all rules combined
combinedReadDf <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(combinedReadDf) <- c("Pattern_rating_Read", "Pattern_TAU", "Antipattern_rating_Read", "Antipattern_TAU")
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varRatingPattern <- paste(p, "RRating", sep = "")
varRatingAntipattern <- paste(ap, "RRating", sep = "")
varTAUPattern <- paste("TAU", p, sep = "_")
varTAUAntipattern <- paste("TAU", ap, sep = "_")
# merge data frames
combinedReadDf <- rbind(combinedReadDf, data.frame(
Pattern_rating = data[[varRatingPattern]],
Pattern_TAU = data[[varTAUPattern]],
Antipattern_rating = data[[varRatingAntipattern]],
Antipattern_TAU = data[[varTAUAntipattern]]
))
}
print(combinedReadDf)
