########################################################################
# Experiment Results
########################################################################
# delete current environment variables --------
rm(list = ls(all.names = TRUE))
# install and load required packages -------
# (taken from https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)
packageList <- c(
"Hmisc", "dplyr", "ggplot2", "tidyr", "corrplot", "likert", "effectsize", "scales", "lmtest"
)
newPackages <- packageList[!(packageList %in% installed.packages()[, "Package"])]
if (length(newPackages)) install.packages(newPackages)
for (p in packageList) {
library(p, character.only = TRUE)
}
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
pwd
getwd()
setwd("~/Linguistic/Journal-Resources/Understanding-Survey")
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# median participant stats by task order group
data %>%
group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numGermany = sum(is_Germany)
)
# read data ------------
data <- read.csv("results.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# distribution of groups
data %>%
select(group_id) %>%
table()
# median participant stats by task order group
data %>%
group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numGermany = sum(is_Germany)
)
# median participant stats by task order group
data %>%
#group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numGermany = sum(is_Germany)
)
# median participant stats by task order group
data %>%
#group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numCanada = sum(is_Canada)
)
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# median participant stats by task order group
data %>%
#group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numCanada = sum(is_Canada)
)
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
# demographic data
# distribution of professions
data %>%
select(Profession) %>%
table()
# distribution of developer perspective
data %>%
select(DeveloperPerspective) %>%
table()
# distribution of country
data %>%
select(Country) %>%
table()
# median participant stats by task order group
data %>%
#group_by(group_id) %>%
summarise(
numParticipants = n(),
medianYearsOfExperience = median(YearsOfExperience, na.rm = TRUE),
numKnowledgeOfRichardsonMaturityModel = sum(RichardsonMaturity, na.rm = TRUE),
medianRichardsonMaturityRating = median(MaturityLevel, na.rm = TRUE),
numStudents = sum(is_Student),
numAcademia = sum(is_Academia),
numIndustry = n() - numStudents - numAcademia,
numCanada = sum(is_Canada)
)
patterns <- c(
"Amorphous", "Contextless", "CRUDy",
"Incosistent", "Non-descriptive", "Non-hierarchical",
"Non-pertient", "Non-standard", "Pluralized",
"Unversioned", "Tunneling", "Archetype", "Ambiguity", "Flat"
)
# variable names for the antipatterns
antipatterns <- c(
"Tidy", "Contextual", "Verbless",
"Cosistent", "Descriptive", "Hierarchical",
"Pertient", "Standard", "Singularized",
"Versioned", "Adheremnce", "Con_Archetype", "Annotation", "Structured"
)
# variable names for the antipatterns
antipatterns <- c(
"Tidy", "Contextual", "Verbless",
"Consistent", "Descriptive", "Hierarchical",
"Pertinent", "Standard", "Singularized",
"Versioned", "Adherence", "Consistent_Archetype", "Annotation", "Structured"
)
# calculate TAU for every question and both groups
for (var in ruleNames) {
var <- as.name(var)
varFR <- as.name(paste(var, "FR", sep = ""))
varFRTime <- as.name(paste(var, "FRTime", sep = ""))
varIR <- as.name(paste(var, "IR", sep = ""))
varIRTime <- as.name(paste(var, "IRTime", sep = ""))
data <- data %>%
mutate("TAUFR_{{var}}" := calculateTAU({{ varFR }}, {{ varFRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE))) %>%
mutate("TAUIR_{{var}}" := calculateTAU({{ varIR }}, {{ varIRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)))
}
# calculate TAU for every question and both groups
for (var in patterns) {
var <- as.name(var)
varFR <- as.name(paste(var, "FR", sep = ""))
varFRTime <- as.name(paste(var, "FRTime", sep = ""))
varIR <- as.name(paste(var, "IR", sep = ""))
varIRTime <- as.name(paste(var, "IRTime", sep = ""))
data <- data %>%
mutate("TAUFR_{{var}}" := calculateTAU({{ varFR }}, {{ varFRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE))) %>%
mutate("TAUIR_{{var}}" := calculateTAU({{ varIR }}, {{ varIRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)))
}
# helper function to calculate the Timed Actual Understandability (TAU)
calculateTAU <- function(effectiveness, efficiency, maxEfficiency) {
effectiveness * (1 - (efficiency / maxEfficiency))
}
# calculate TAU for every question and both groups
for (var in patterns) {
var <- as.name(var)
varFR <- as.name(paste(var, "FR", sep = ""))
varFRTime <- as.name(paste(var, "FRTime", sep = ""))
varIR <- as.name(paste(var, "IR", sep = ""))
varIRTime <- as.name(paste(var, "IRTime", sep = ""))
data <- data %>%
mutate("TAUFR_{{var}}" := calculateTAU({{ varFR }}, {{ varFRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE))) %>%
mutate("TAUIR_{{var}}" := calculateTAU({{ varIR }}, {{ varIRTime }}, max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)))
}
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "FRTime", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "IRTime", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "Time", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "Time", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "Time", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "Time", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "Time", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "Time", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
# variable names for the patterns
patterns <- c(
"Amorphous", "Contextless", "CRUDy",
"Inconsistent", "Non-descriptive", "Non-hierarchical",
"Non-pertient", "Non-standard", "Pluralized",
"Unversioned", "Tunneling", "Inconsistent_Archetype", "Ambiguity", "Flat"
)
# variable names for the antipatterns
antipatterns <- c(
"Tidy", "Contextual", "Verbless",
"Consistent", "Descriptive", "Hierarchical",
"Pertinent", "Standard", "Singularized",
"Versioned", "Adherence", "Consistent_Archetype", "Annotation", "Structured"
)
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "Time", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "Time", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "Time", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "Time", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
# read data ------------
data <- read.csv("results_survey.csv", na.strings = c("", "NA"), fileEncoding = "UTF-8-BOM")
#Iterate through indices of both vectors
for (i in seq_along(patterns)) {
p <- patterns[i]
ap <- antipatterns[i]
varFR <- as.name(p)
varFRTime <- as.name(paste(p, "Time", sep = ""))
varIR <- as.name(ap)
varIRTime <- as.name(paste(ap, "Time", sep = ""))
# Add TAU calculations to the data frame
data <- data %>%
mutate(
!!paste0("TAUFR_", p) := calculateTAU(
{{ varFR }},
{{ varFRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
) %>%
mutate(
!!paste0("TAUIR_", ap) := calculateTAU(
{{ varIR }},
{{ varIRTime }},
max({{ varFRTime }}, {{ varIRTime }}, na.rm = TRUE)
)
)
}
